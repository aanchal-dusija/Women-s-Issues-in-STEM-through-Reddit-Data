{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505201162
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505201646
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%configure -f \\\n",
        "{\"conf\": {\"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.2\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install azureml.fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505430389
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import io\n",
        "from azureml.fsspec import AzureMachineLearningFileSystem\n",
        "\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "# from azureml.core import AzureMachineLearningFileSystem\n",
        "import io\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext \n",
        "from pyspark.sql.functions import col, lower, count, length, unix_timestamp, current_timestamp, to_date, desc\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import seaborn as sns\n",
        "from pyspark.sql import functions as F\n",
        "from scipy.stats import tstd\n",
        "import nltk\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "# !pip install plotly\n",
        "# !pip install wordcloud\n",
        "import plotly.express as px\n",
        "\n",
        "# download the nltk stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql import Window\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF, HashingTF, StopWordsRemover\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, MapType, StringType\n",
        "import string\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import SQLTransformer\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer, Normalizer, StopWordsCleaner, LemmatizerModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505465300
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "\n",
        "# Azure Storage Account details\n",
        "storage_account_name = \"group23astorage87e07d16d\"\n",
        "storage_account_access_key = \"gkm2Ao5WwFH2W/6Udmx14mJyvglx8W74in3UXd8JrARi99bf7UFdFxnAJKnI5w4gk6ePiKTmH5XL+AStoA70xw==\"\n",
        "container_name = \"azureml-blobstore-b5f84dac-6a3c-4f67-8b42-7075c37e11ae\"\n",
        "\n",
        "# Configure Spark to access Azure Blob Storage\n",
        "conf = SparkConf()\n",
        "conf.set(\"fs.azure.account.key.\" + storage_account_name + \".blob.core.windows.net\", storage_account_access_key)\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"AzureBlobStorageRead\").config(conf=conf).getOrCreate()\n",
        "\n",
        "# Years to process\n",
        "years = ['2021', '2022', '2023']\n",
        "\n",
        "# Initialize an empty list to hold DataFrames\n",
        "all_dfs = []\n",
        "\n",
        "# Process each year\n",
        "for year in years:\n",
        "    # Specify the file path in the container for the current year\n",
        "    file_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/cleaned/comments/yyyy={year}/*.parquet\"\n",
        "\n",
        "    # Read the parquet files into a DataFrame\n",
        "    df = spark.read.format(\"parquet\").load(file_path)\n",
        "\n",
        "    # Append the DataFrame to the list\n",
        "    all_dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames for all years\n",
        "comments = all_dfs[0]\n",
        "for df in all_dfs[1:]:\n",
        "    comments = comments.union(df)\n",
        "\n",
        "# Show the selected columns\n",
        "comments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\", \"score\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505479795
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "\n",
        "# Azure Storage Account details\n",
        "storage_account_name = \"group23astorage87e07d16d\"\n",
        "storage_account_access_key = \"gkm2Ao5WwFH2W/6Udmx14mJyvglx8W74in3UXd8JrARi99bf7UFdFxnAJKnI5w4gk6ePiKTmH5XL+AStoA70xw==\"\n",
        "container_name = \"azureml-blobstore-b5f84dac-6a3c-4f67-8b42-7075c37e11ae\"\n",
        "\n",
        "# Configure Spark to access Azure Blob Storage\n",
        "conf = SparkConf()\n",
        "conf.set(\"fs.azure.account.key.\" + storage_account_name + \".blob.core.windows.net\", storage_account_access_key)\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"AzureBlobStorageRead\").config(conf=conf).getOrCreate()\n",
        "\n",
        "# Years to process\n",
        "years = ['2021', '2022', '2023']\n",
        "\n",
        "# Initialize an empty list to hold DataFrames\n",
        "all_dfs = []\n",
        "\n",
        "# Process each year\n",
        "for year in years:\n",
        "    # Specify the file path in the container for the current year\n",
        "    file_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/cleaned/submissions/yyyy={year}/*.parquet\"\n",
        "\n",
        "    # Read the parquet files into a DataFrame\n",
        "    df = spark.read.format(\"parquet\").load(file_path)\n",
        "\n",
        "    # Append the DataFrame to the list\n",
        "    all_dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames for all years\n",
        "submissions = all_dfs[0]\n",
        "for df in all_dfs[1:]:\n",
        "    submissions = submissions.union(df)\n",
        "\n",
        "# Select specific columns\n",
        "submissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"created_utc\", \"num_comments\", \"score\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Filter \"AskWomen\", \"AskFeminists\", \"Feminism\" by STEM Keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505484198
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Subreddits to filter by keywords\n",
        "keyword_subreddits = [\"AskWomen\", \"AskFeminists\", \"Feminism\"]\n",
        "# Subreddits to include all comments from\n",
        "include_all_subreddits = [\"xxstem\", \"LadiesofScience\", \"womenEngineers\"]\n",
        "\n",
        "# Define keywords for case-insensitive search\n",
        "keywords = [\"STEM\", \"Science\", \"Technology\", \"Engineering\", \"Mathematics\", \"Process\", \"Design\", \"Model\", \"Plan\", \"Project\"]\n",
        "keywords_lower = [kw.lower() for kw in keywords]\n",
        "\n",
        "# Filter the DataFrame\n",
        "comments = comments.filter(\n",
        "    (col(\"subreddit\").isin(keyword_subreddits) & col(\"body\").rlike('|'.join(keywords_lower))) |\n",
        "    (col(\"subreddit\").isin(include_all_subreddits))\n",
        ")\n",
        "\n",
        "# Show the filtered data\n",
        "comments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\", \"score\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505491072
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Subreddits to filter by keywords\n",
        "keyword_subreddits = [\"AskWomen\", \"AskFeminists\", \"Feminism\"]\n",
        "# Subreddits to include all submissions from\n",
        "include_all_subreddits = [\"xxstem\", \"LadiesofScience\", \"womenEngineers\"]\n",
        "\n",
        "# Define keywords for case-insensitive search\n",
        "keywords = [\"STEM\", \"science\", \"technology\", \"engineering\", \"mathematics\", \"process\", \"design\", \"model\", \"plan\", \"project\"]\n",
        "# Create a regex pattern to match any keyword (case-insensitive)\n",
        "pattern = '|'.join([f\"(?i){kw}\" for kw in keywords])\n",
        "\n",
        "# Filter the DataFrame\n",
        "# Include all submissions from certain subreddits or those that match the keyword pattern in their title or selftext\n",
        "submissions = submissions.filter(\n",
        "    (col(\"subreddit\").isin(keyword_subreddits) & (col(\"title\").rlike(pattern) | col(\"selftext\").rlike(pattern))) |\n",
        "    col(\"subreddit\").isin(include_all_subreddits)\n",
        ")\n",
        "\n",
        "# Show the filtered data\n",
        "submissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"created_utc\", \"num_comments\", \"score\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Cleaning the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505498545
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Remove rows with missing values in 'author' and 'body'\n",
        "comments = comments.filter(col(\"author\").isNotNull() & col(\"body\").isNotNull())\n",
        "\n",
        "# Assume that 'created_utc' should be a timestamp within the last 3 years\n",
        "three_years_ago = unix_timestamp(current_timestamp()) - (3 * 365 * 24 * 60 * 60)\n",
        "comments = comments.filter(\n",
        "    unix_timestamp(col(\"created_utc\")) > three_years_ago\n",
        ")\n",
        "\n",
        "# Show the filtered data\n",
        "comments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\", \"score\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505510695
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Remove rows with missing values in 'author' or 'title'\n",
        "submissions = submissions.filter(col(\"author\").isNotNull() & col(\"title\").isNotNull())\n",
        "\n",
        "# Assume that 'created_utc' should be a timestamp within the last 3 years\n",
        "three_years_ago = unix_timestamp(current_timestamp()) - (3 * 365 * 24 * 60 * 60)\n",
        "submissions = submissions.filter(\n",
        "    unix_timestamp(col(\"created_utc\")) > three_years_ago\n",
        ")\n",
        "\n",
        "# Show the filtered data\n",
        "submissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"created_utc\", \"num_comments\",  \"score\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# External Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700435904198
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.storage.blob import BlobServiceClient\n",
        "import pandas as pd\n",
        "\n",
        "# Azure Storage Account details\n",
        "storage_account_name = \"group23astorage87e07d16d\"\n",
        "storage_account_key = \"gkm2Ao5WwFH2W/6Udmx14mJyvglx8W74in3UXd8JrARi99bf7UFdFxnAJKnI5w4gk6ePiKTmH5XL+AStoA70xw==\"\n",
        "container_name = \"azureml-blobstore-b5f84dac-6a3c-4f67-8b42-7075c37e11ae\"\n",
        "blob_name = \"nsf23315-report-figures-tables-excels/nsf23315-fig001-001.xlsx\"\n",
        "\n",
        "# Create a blob client\n",
        "blob_service_client = BlobServiceClient(account_url=f\"https://{storage_account_name}.blob.core.windows.net\", credential=storage_account_key)\n",
        "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "\n",
        "# Download the blob to a local file\n",
        "local_file_name = \"downloaded_nsf23315_fig001_001.xlsx\"\n",
        "with open(local_file_name, \"wb\") as download_file:\n",
        "    download_file.write(blob_client.download_blob().readall())\n",
        "\n",
        "# Read the Excel file using pandas\n",
        "df = pd.read_excel(local_file_name)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700436632409
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "df_spark = spark.createDataFrame(df)\n",
        "\n",
        "df_spark = df_spark.withColumn(\"dummy_key\", lit(1))\n",
        "comments = comments.withColumn(\"dummy_key\", lit(1))\n",
        "\n",
        "merged_df = comments.join(df_spark, \"dummy_key\")\n",
        "\n",
        "merged_df = merged_df.drop(\"dummy_key\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700437581282
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Summary by Subreddit\n",
        "summary_subreddit = merged_df.groupBy(\"subreddit\").agg(\n",
        "    F.count(\"id\").alias(\"comment_count\"),\n",
        "    F.avg(\"score\").alias(\"average_score\"),\n",
        "    F.avg(\"Total population\").alias(\"avg_total_population\")\n",
        ")\n",
        "\n",
        "summary_subreddit.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Most Frequent Words in the Comments and Submissions Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505513094
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.sql.functions import udf, col, explode, array_union\n",
        "from pyspark.sql.types import StringType\n",
        "import re\n",
        "\n",
        "# Define a UDF for cleaning text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "clean_text_udf = udf(clean_text, StringType())\n",
        "\n",
        "# Preprocess and tokenize text for both datasets\n",
        "comments = comments.withColumn(\"cleaned_body\", clean_text_udf(\"body\"))\n",
        "submissions = submissions.withColumn(\"cleaned_title\", clean_text_udf(\"title\"))\n",
        "submissions = submissions.withColumn(\"cleaned_selftext\", clean_text_udf(\"selftext\"))\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"cleaned_body\", outputCol=\"words\")\n",
        "comments = tokenizer.transform(comments)\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"cleaned_title\", outputCol=\"title_words\")\n",
        "submissions = tokenizer.transform(submissions)\n",
        "tokenizer = Tokenizer(inputCol=\"cleaned_selftext\", outputCol=\"selftext_words\")\n",
        "submissions = tokenizer.transform(submissions)\n",
        "\n",
        "# Define additional stop words\n",
        "additional_stop_words = [\"like\", \"dont\", \"im\", \"one\", \"removed\", \"get\", \"also\", \"even\", \"really\", \"sa\", \"despite\", \"certainly\"]\n",
        "\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "remover.setStopWords(remover.getStopWords() + additional_stop_words)\n",
        "comments = remover.transform(comments)\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"title_words\", outputCol=\"filtered_title_words\")\n",
        "remover.setStopWords(remover.getStopWords() + additional_stop_words)\n",
        "submissions = remover.transform(submissions)\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"selftext_words\", outputCol=\"filtered_selftext_words\")\n",
        "remover.setStopWords(remover.getStopWords() + additional_stop_words)\n",
        "submissions = remover.transform(submissions)\n",
        "\n",
        "# Combine title and selftext words for submissions\n",
        "submissions = submissions.withColumn(\"combined_words\", array_union(\"filtered_title_words\", \"filtered_selftext_words\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Finding Important Keywords and creating dummies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Education and Color\n",
        "edu_keywords = ['education', 'school', 'university']\n",
        "col_keywords = ['\"hispanic\"', 'black', 'asian', 'ethnicity']\n",
        "\n",
        "# Regular expressions for the topics\n",
        "edu_regex = '|'.join([re.escape(word) for word in edu_keywords])\n",
        "col_regex = '|'.join([re.escape(word) for word in col_keywords])\n",
        "\n",
        "# Define UDFs to check for the presence of topic keywords\n",
        "def contains_tech(text):\n",
        "    return int(bool(re.search(edu_regex, text.lower())))\n",
        "\n",
        "def contains_sports(text):\n",
        "    return int(bool(re.search(col_regex, text.lower())))\n",
        "\n",
        "contains_edu_udf = udf(contains_tech, IntegerType())\n",
        "contains_col_udf = udf(contains_sports, IntegerType())\n",
        "\n",
        "# Apply UDFs to create dummy variables\n",
        "comments = comments.withColumn(\"is_edu\", contains_edu_udf(\"cleaned_body\"))\n",
        "comments = comments.withColumn(\"is_col\", contains_col_udf(\"cleaned_body\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Calculating Word Frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505601236
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Calculate Word Frequencies\n",
        "comments_exploded = comments.withColumn(\"word\", explode(\"filtered_words\"))\n",
        "comments_exploded = comments_exploded.filter(col(\"word\").rlike(r'\\S'))\n",
        "\n",
        "submissions_exploded = submissions.withColumn(\"word\", explode(\"combined_words\"))\n",
        "submissions_exploded = submissions_exploded.filter(col(\"word\").rlike(r'\\S'))\n",
        "\n",
        "comments_word_freq = comments_exploded.groupBy(\"word\").count().sort(\"count\", ascending=False)\n",
        "submissions_word_freq = submissions_exploded.groupBy(\"word\").count().sort(\"count\", ascending=False)\n",
        "\n",
        "# Display the tables\n",
        "comments_word_freq.show(10)\n",
        "submissions_word_freq.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install pandas tabulate pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700346572296
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "comments_word_freq_pd = comments_word_freq.toPandas()\n",
        "submissions_word_freq_pd = submissions_word_freq.toPandas()\n",
        "\n",
        "# Function to convert DataFrame to image\n",
        "def df_to_image(df, filename):\n",
        "    # Convert DataFrame to table string\n",
        "    table_str = tabulate(df.head(10), headers='keys', tablefmt='plain')\n",
        "\n",
        "    # Create an image with white background\n",
        "    img = Image.new('RGB', (1000, 300), color = (255, 255, 255))\n",
        "    d = ImageDraw.Draw(img)\n",
        "\n",
        "    # Use a truetype font (you might need to adjust the path to the font file)\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "    # Draw the table on the image\n",
        "    d.multiline_text((10,10), table_str, fill=(0,0,0), font=font)\n",
        "\n",
        "    # Save the image\n",
        "    img.save(filename)\n",
        "\n",
        "# Convert DataFrames to images\n",
        "df_to_image(comments_word_freq_pd, 'comments_word_freq.png') # change path\n",
        "df_to_image(submissions_word_freq_pd, 'submissions_word_freq.png') # change path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Word Clouds after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700505532311
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "comments_pd = comments_word_freq.toPandas()\n",
        "submissions_pd = submissions_word_freq.toPandas()\n",
        "\n",
        "# Create a dictionary of word frequencies for word cloud\n",
        "comments_dict = dict(zip(comments_pd['word'], comments_pd['count']))\n",
        "submissions_dict = dict(zip(submissions_pd['word'], submissions_pd['count']))\n",
        "\n",
        "# Generate word cloud for comments\n",
        "wordcloud_comments = WordCloud(width = 800, height = 800, \n",
        "                background_color ='white', \n",
        "                min_font_size = 10).generate_from_frequencies(comments_dict)\n",
        "\n",
        "# Plot and save the WordCloud image for comments                       \n",
        "plt.figure(figsize = (8, 8), facecolor = None) \n",
        "plt.imshow(wordcloud_comments) \n",
        "plt.axis(\"off\") \n",
        "plt.tight_layout(pad = 0) \n",
        "plt.savefig('wordcloud_comments.png')  # CHANGE PATH\n",
        "plt.show()  # Close the plot to avoid displaying it in the notebook\n",
        "\n",
        "# Generate word cloud for submissions\n",
        "wordcloud_submissions = WordCloud(width = 800, height = 800, \n",
        "                background_color ='white', \n",
        "                min_font_size = 10).generate_from_frequencies(submissions_dict)\n",
        "\n",
        "# Plot and save the WordCloud image for submissions                       \n",
        "plt.figure(figsize = (8, 8), facecolor = None) \n",
        "plt.imshow(wordcloud_submissions) \n",
        "plt.axis(\"off\") \n",
        "plt.tight_layout(pad = 0) \n",
        "plt.savefig('wordcloud_submissions.png')  \n",
        "plt.show()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Average distribution of text lengths for Submissions and Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700460362042
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "submissions.createOrReplaceTempView(\"submissions\")\n",
        "comments.createOrReplaceTempView(\"comments\")\n",
        "\n",
        "avg_subm_length_by_month = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        subreddit,\n",
        "        date_format(created_utc, \"yyyy-MM\") AS month,\n",
        "        AVG(LENGTH(selftext)) AS avg_subm_length\n",
        "    FROM submissions\n",
        "    GROUP BY subreddit, month\n",
        "    ORDER BY month\n",
        "\"\"\")\n",
        "\n",
        "avg_comm_length_by_month = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        subreddit,\n",
        "        date_format(created_utc, \"yyyy-MM\") AS month,\n",
        "        AVG(LENGTH(body)) AS avg_comm_length\n",
        "    FROM comments\n",
        "    GROUP BY subreddit, month\n",
        "    ORDER BY month\n",
        "\"\"\")\n",
        "\n",
        "# Convert the Spark DataFrames to Pandas DataFrames\n",
        "avg_subm_length_by_month = avg_subm_length_by_month.toPandas()\n",
        "avg_comm_length_by_month = avg_comm_length_by_month.toPandas()\n",
        "\n",
        "# Create a line chart of average submission length by month\n",
        "fig = px.line(\n",
        "    avg_subm_length_by_month,\n",
        "    x=\"month\",\n",
        "    y=\"avg_subm_length\",\n",
        "    color=\"subreddit\",\n",
        "    title=\"Average Submission Length by Month\",\n",
        "    labels={\"avg_subm_length\": \"Average Submission Length\"}\n",
        ")\n",
        "\n",
        "# Save the figure as an HTML file\n",
        "fig.write_html(\"average_text_length_over_time_sub.html\")\n",
        "\n",
        "\n",
        "# Display the charts\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700460450305
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create a line chart of average comment length by month\n",
        "fig = px.line(\n",
        "    avg_comm_length_by_month,\n",
        "    x=\"month\",\n",
        "    y=\"avg_comm_length\",\n",
        "    color=\"subreddit\",\n",
        "    title=\"Average Comment Length by Month\",\n",
        "    labels={\"avg_comm_length\": \"Average Comments Length\"}\n",
        ")\n",
        "\n",
        "# Save the figure as an HTML file\n",
        "fig.write_html(\"average_text_length_over_time_com.html\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700432713472
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.sql.functions import udf, explode, col, max\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "\n",
        "# Count Vectorizer and IDF for comments\n",
        "cv_comments = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
        "cv_model_comments = cv_comments.fit(comments)\n",
        "comments_featurized = cv_model_comments.transform(comments)\n",
        "\n",
        "idf_comments = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "idf_model_comments = idf_comments.fit(comments_featurized)\n",
        "comments_tfidf = idf_model_comments.transform(comments_featurized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700432925324
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "comments_tfidf.select(\"filtered_words\", \"raw_features\", \"features\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700432760075
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "\n",
        "# Count Vectorizer and IDF for submissions\n",
        "cv_submissions = CountVectorizer(inputCol=\"combined_words\", outputCol=\"raw_features\")\n",
        "cv_model_submissions = cv_submissions.fit(submissions)\n",
        "submissions_featurized = cv_model_submissions.transform(submissions)\n",
        "\n",
        "idf_submissions = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "idf_model_submissions = idf_submissions.fit(submissions_featurized)\n",
        "submissions_tfidf = idf_model_submissions.transform(submissions_featurized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700432896602
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "submissions_tfidf.select(\"combined_words\", \"raw_features\", \"features\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# TF-IDF SCORE CALCULATION FOR SUBMISSIONS AND COMMENTS DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700432766860
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.sql.functions import udf, col, explode, array_union, monotonically_increasing_id\n",
        "from pyspark.sql.types import StringType, ArrayType, DoubleType\n",
        "import re\n",
        "\n",
        "# UDF to extract TF-IDF values from SparseVector - submissions and comments\n",
        "def extract_tf_idf_values(features):\n",
        "    return features.toArray().tolist()\n",
        "\n",
        "extract_tf_idf_values_udf = udf(extract_tf_idf_values, ArrayType(DoubleType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700432777628
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Apply UDF to create a new column with TF-IDF values\n",
        "comments_tfidf = comments_tfidf.withColumn(\"tf_idf_values\", extract_tf_idf_values_udf(\"features\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700432983219
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "comments_tfidf.select(\"filtered_words\", \"raw_features\", \"features\", \"tf_idf_values\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700433150617
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "submissions_tfidf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700430439975
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "comments_tfidf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700433241229
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "comments_exploded_tfidf = comments_tfidf.select(\"filtered_words\", \"tf_idf_values\").withColumn(\"words\", explode(\"filtered_words\")).withColumn(\"tf_idf\", explode(\"tf_idf_values\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700433299512
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "comments_exploded_tfidf.select(\"filtered_words\", \"tf_idf_values\", \"words\", \"tf_idf\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700431229207
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Filter out words that are URLs\n",
        "# Explode the words along with their indices\n",
        "comments_exploded = comments_with_index.select(\n",
        "    posexplode(\"indexed_words\").alias(\"idx\", \"word_with_index\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700431691858
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Extract the word and its corresponding TF-IDF value\n",
        "comments_exploded_tfidf = comments_exploded.select(\n",
        "    col(\"word_with_index.word\").alias(\"word\"),\n",
        "    col(\"tf_idf_values\")[col(\"idx\")].alias(\"tf_idf\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700416205063
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Group by word and find the maximum TF-IDF value for each word\n",
        "top_comments_tf_idf = filtered_comments_tfidf.groupBy(\"word\").agg(max(\"tf_idf\").alias(\"max_tf_idf\")).orderBy(col(\"max_tf_idf\").desc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700433018695
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.sql.functions import udf, col, explode, array_union, monotonically_increasing_id, row_number\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StringType, ArrayType, DoubleType\n",
        "import re\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.sql.functions import udf, col, explode, array_union, max\n",
        "from pyspark.sql.types import StringType, ArrayType, DoubleType\n",
        "import re\n",
        "\n",
        "submissions_tfidf = submissions_tfidf.withColumn(\"tf_idf_values\", extract_tf_idf_values_udf(\"features\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700433041804
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "submissions_tfidf.select(\"combined_words\", \"raw_features\", \"features\", \"tf_idf_values\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700433349596
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "submissions_exploded_tfidf = submissions_tfidf.select(\"combined_words\", \"tf_idf_values\").withColumn(\"word\", explode(\"combined_words\")).withColumn(\"tf_idf\", explode(\"tf_idf_values\"))\n",
        "submissions_exploded_tfidf = submissions_exploded_tfidf.filter(~col(\"word\").rlike(\"^(http|https)\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700433731594
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "submissions_exploded_tfidf.select(\"combined_words\", \"tf_idf_values\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Group by word and find the maximum TF-IDF value for each word\n",
        "top_submissions_tf_idf = submissions_exploded_tfidf.groupBy(\"word\").agg(max(\"tf_idf\").alias(\"max_tf_idf\")).orderBy(col(\"max_tf_idf\").desc())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700416519086
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Show top results\n",
        "top_comments_tf_idf.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700364328003
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Display the results\n",
        "top_submissions_tf_idf.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Sentiment Analysis for Comments and Submissions Datasets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Twitter Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700425545662
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import UniversalSentenceEncoder, SentimentDLModel\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import sparknlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700425560135
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Assuming 'comments' and 'submissions' are your DataFrames and they have a column 'text' for analysis\n",
        "comments = comments.withColumnRenamed(\"body\", \"text\")\n",
        "# submissions = submissions.withColumnRenamed(\"title\", \"text\")\n",
        "\n",
        "# Define the pipeline\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\") \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "sentimentdl = SentimentDLModel.pretrained(name=\"sentimentdl_use_twitter\", lang=\"en\") \\\n",
        "    .setInputCols([\"sentence_embeddings\"]) \\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "\n",
        "sentiment_pipeline1 = Pipeline(stages=[documentAssembler, use, sentimentdl])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "model1 = sentiment_pipeline1.fit(comments)\n",
        "\n",
        "# Transform the data\n",
        "result1 = model1.transform(comments)\n",
        "\n",
        "# Show some results\n",
        "# result.select(\"text\", \"sentiment.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700425572363
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Concatenate 'title' and 'selftext' into a single text column if needed\n",
        "submissions = submissions.withColumn(\"text\", F.concat_ws(\" \", submissions.title, submissions.selftext))\n",
        "\n",
        "# Define the pipeline (same as before)\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\") \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "sentimentdl = SentimentDLModel.pretrained(name=\"sentimentdl_use_twitter\", lang=\"en\") \\\n",
        "    .setInputCols([\"sentence_embeddings\"]) \\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "\n",
        "sentiment_pipeline2 = Pipeline(stages=[documentAssembler, use, sentimentdl])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "model2 = sentiment_pipeline2.fit(submissions)\n",
        "\n",
        "# Transform the data\n",
        "result2 = model2.transform(submissions)\n",
        "\n",
        "# Show some results\n",
        "# result.select(\"text\", \"sentiment.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Vivek Sentiment Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700425690935
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.base import DocumentAssembler, Finisher\n",
        "from sparknlp.annotator import Tokenizer, Normalizer, ViveknSentimentModel\n",
        "import sparknlp\n",
        "\n",
        "# Define the pipeline\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"tokenized\")\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"tokenized\"]) \\\n",
        "    .setOutputCol(\"normalized\")\n",
        "\n",
        "vivekn = ViveknSentimentModel.pretrained() \\\n",
        "    .setInputCols([\"document\", \"normalized\"]) \\\n",
        "    .setOutputCol(\"vn_sentiment\")\n",
        "\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"vn_sentiment\"]) \\\n",
        "    .setOutputCols([\"sentiment\"])\n",
        "\n",
        "sentiment_pipeline3 = Pipeline(stages=[documentAssembler, tokenizer, normalizer, vivekn, finisher])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "model3 = sentiment_pipeline3.fit(comments)\n",
        "\n",
        "# Transform the data\n",
        "result3 = model3.transform(comments)\n",
        "\n",
        "# Show some results\n",
        "# result.select(\"body\", \"sentiment\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700428641214
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "result3.select(\"text\", \"sentiment\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700425704963
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Prepare your data for the 'submissions' DataFrame\n",
        "submissions = submissions.withColumn(\"text\", F.concat_ws(\" \", submissions.title, submissions.selftext))\n",
        "\n",
        "# The pipeline remains the same, just change the input column for the DocumentAssembler\n",
        "documentAssembler.setInputCol(\"text\")\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "model4 = sentiment_pipeline3.fit(submissions)\n",
        "\n",
        "# Transform the data\n",
        "result4 = model4.transform(submissions)\n",
        "\n",
        "# Show some results\n",
        "# result.select(\"text\", \"sentiment\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# IMDB Sentiment Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700425762826
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import UniversalSentenceEncoder, SentimentDLModel\n",
        "import sparknlp\n",
        "\n",
        "# Initialize Spark NLP\n",
        "spark = sparknlp.start()\n",
        "\n",
        "MODEL_3 = \"sentimentdl_use_imdb\"\n",
        "\n",
        "# Define the pipeline\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\") \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "sentimentDl = SentimentDLModel.pretrained(name=MODEL_3, lang='en') \\\n",
        "    .setInputCols([\"sentence_embeddings\"]) \\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "\n",
        "sentiment_pipeline4 = Pipeline(stages=[documentAssembler, use, sentimentDl])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "model5 = sentiment_pipeline4.fit(comments)\n",
        "\n",
        "# Transform the data\n",
        "result5 = model5.transform(comments)\n",
        "\n",
        "# Show some results\n",
        "# result.select(\"body\", \"sentiment\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700425783368
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Prepare your data for the 'submissions' DataFrame\n",
        "submissions = submissions.withColumn(\"text\", F.concat_ws(\" \", submissions.title, submissions.selftext))\n",
        "\n",
        "# The pipeline remains the same, just change the input column for the DocumentAssembler\n",
        "documentAssembler.setInputCol(\"text\")\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "model6 = sentiment_pipeline4.fit(submissions)\n",
        "\n",
        "# Transform the data\n",
        "result6 = model6.transform(submissions)\n",
        "\n",
        "# Show some results\n",
        "# result.select(\"text\", \"sentiment\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700428231259
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Explode the sentiment results for the 'comments' DataFrame\n",
        "exploded_results1 = result1.withColumn(\"sentiment\", explode(\"sentiment.result\"))\n",
        "exploded_results2 = result2.withColumn(\"sentiment\", explode(\"sentiment.result\"))\n",
        "# exploded_results3 = result3.withColumn(\"sentiment\", explode(\"sentiment.result\"))\n",
        "# exploded_results4 = result4.withColumn(\"sentiment\", explode(\"sentiment.result\"))\n",
        "exploded_results5 = result5.withColumn(\"sentiment\", explode(\"sentiment.result\"))\n",
        "exploded_results6 = result6.withColumn(\"sentiment\", explode(\"sentiment.result\"))\n",
        "\n",
        "from pyspark.sql.functions import explode\n",
        "sentiment_counts1 = exploded_results1.groupBy(\"sentiment\").count()\n",
        "sentiment_counts2 = exploded_results2.groupBy(\"sentiment\").count()\n",
        "# sentiment_counts3 = exploded_results3.groupBy(\"sentiment\").count()\n",
        "# sentiment_counts4 = exploded_results4.groupBy(\"sentiment\").count()\n",
        "sentiment_counts5 = exploded_results5.groupBy(\"sentiment\").count()\n",
        "sentiment_counts6 = exploded_results6.groupBy(\"sentiment\").count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700428428243
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode, col, lit\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum as _sum\n",
        "\n",
        "# Function to calculate and show sentiment percentages\n",
        "def show_sentiment_percentages(results):\n",
        "    # Explode the sentiment results\n",
        "    exploded_results = results.withColumn(\"sentiment\", explode(\"sentiment.result\"))\n",
        "\n",
        "    # Count the total number of sentiments\n",
        "    total_count = exploded_results.count()\n",
        "\n",
        "    # Calculate the count of each sentiment type\n",
        "    sentiment_counts = exploded_results.groupBy(\"sentiment\").count()\n",
        "\n",
        "    # Calculate the percentage of each sentiment type\n",
        "    sentiment_percentages = sentiment_counts.withColumn(\"percentage\", col(\"count\") / lit(total_count) * 100)\n",
        "\n",
        "    # Show the sentiment percentages\n",
        "    sentiment_percentages.show()\n",
        "\n",
        "# Apply the function to the results for 'comments'\n",
        "\n",
        "show_sentiment_percentages(result1)\n",
        "show_sentiment_percentages(result2)\n",
        "# show_sentiment_percentages(result3)\n",
        "# show_sentiment_percentages(result4)\n",
        "show_sentiment_percentages(result5)\n",
        "show_sentiment_percentages(result6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700429070856
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, lit\n",
        "\n",
        "# Function to calculate and show sentiment percentages\n",
        "def calculate_sentiment_percentages(result):\n",
        "    # Count the total number of sentiments\n",
        "    total_count = result.count()\n",
        "\n",
        "    # Calculate the count of each sentiment type\n",
        "    sentiment_counts = result.groupBy(\"sentiment\").count()\n",
        "\n",
        "    # Calculate the percentage of each sentiment type\n",
        "    sentiment_percentages = sentiment_counts.withColumn(\"percentage\", col(\"count\") / lit(total_count) * 100)\n",
        "\n",
        "    return sentiment_percentages\n",
        "\n",
        "# Calculate sentiment percentages for result3\n",
        "sentiment_percentages3 = calculate_sentiment_percentages(result3)\n",
        "sentiment_percentages3.show()\n",
        "\n",
        "# Calculate sentiment percentages for result4\n",
        "sentiment_percentages4 = calculate_sentiment_percentages(result4)\n",
        "sentiment_percentages4.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Disclaimer: The work has been by us. However, we have taken assissstance from ChatGPT for code commenting and cleaning."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "codemirror_mode": "ipython",
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython",
      "version": "3.8.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
