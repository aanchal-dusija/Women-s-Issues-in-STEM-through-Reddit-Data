{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We need an available Java installation to run pyspark. The easiest way to do this is to install JDK and set the proper paths using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.2.0\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql.functions import col, lower, count, length, unix_timestamp, current_timestamp, to_date, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "from pyspark.sql import functions as F\n",
    "from scipy.stats import tstd\n",
    "import nltk\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "# !pip install plotly\n",
    "# !pip install wordcloud\n",
    "import plotly.express as px\n",
    "\n",
    "# download the nltk stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process S3 data with SageMaker Processing Job `PySparkProcessor`\n",
    "\n",
    "We are going to move the above processing code in a Python file and then submit that file to SageMaker Processing Job's [`PySparkProcessor`](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#pysparkprocessor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./code/process.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")    \n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n",
    "    parser.add_argument(\"--col_name_for_filtering\", type=str, help=\"Name of the column to filter\")\n",
    "    parser.add_argument(\"--values_to_keep\", type=str, help=\"comma separated list of values to keep in the filtered set\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "    logger.info(f\"spark version = {spark.version}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # filter the dataframe to only keep the values of interest\n",
    "    vals = [s.strip() for s in args.values_to_keep.split(\",\")]\n",
    "    df_filtered = df.where(col(args.col_name_for_filtering).isin(vals))\n",
    "    \n",
    "    # save the filtered dataframes so that these files can now be used for future analysis\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}\"\n",
    "    logger.info(f\"going to write data for {vals} in {s3_path}\")\n",
    "    logger.info(f\"shape of the df_filtered dataframe is {df_filtered.count():,}x{len(df_filtered.columns)}\")\n",
    "    df_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    \n",
    "    logger.info(f\"all done...\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now submit this code to SageMaker Processing Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "subreddits = \"socialism, Economics\"\n",
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\"spark.executor.memory\": \"12g\", \"spark.executor.cores\": \"4\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "# the dataset contains data for these 3 years\n",
    "year_list = [2021,2022,2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for yyyy in year_list:\n",
    "    print(f\"going to filter comments data for year={yyyy}\")\n",
    "    s3_dataset_path_commments = f\"s3://bigdatateaching/reddit-parquet/comments/year={yyyy}/month=*/*.parquet\" # \"s3a://bigdatateaching/reddit/parquet/comments/yyyy=*/mm=*/*comments*.parquet\"\n",
    "    output_prefix_data_comments = f\"project/comments/yyyy={yyyy}\"\n",
    "    col_name_for_filtering = \"subreddit\"\n",
    "    subreddits = \"socialism, Economics\"\n",
    "\n",
    "    # run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "    spark_processor.run(\n",
    "        submit_app=\"./code/process.py\",\n",
    "        arguments=[\n",
    "            \"--s3_dataset_path\",\n",
    "            s3_dataset_path_commments,\n",
    "            \"--s3_output_bucket\",\n",
    "            bucket,\n",
    "            \"--s3_output_prefix\",\n",
    "            output_prefix_data_comments,\n",
    "            \"--col_name_for_filtering\",\n",
    "            col_name_for_filtering,\n",
    "            \"--values_to_keep\",\n",
    "            subreddits,\n",
    "        ],\n",
    "        spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "        logs=False,\n",
    "        configuration=configuration\n",
    "    )\n",
    "    # give some time for resources from this iterations to get cleaned up\n",
    "    # if we start the job immediately we could get insufficient resources error\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for yyyy in year_list:\n",
    "    print(f\"going to filter submissions data for year={yyyy}\")\n",
    "    s3_dataset_path_submissions = f\"s3://bigdatateaching/reddit-parquet/submissions/year={yyyy}/month=*/*.parquet\" # \"s3a://bigdatateaching/reddit/parquet/submissions/yyyy=*/mm=*/*submissions*.parquet\"\n",
    "    output_prefix_data_submissions = f\"project/submissions/yyyy={yyyy}\"\n",
    "\n",
    "    # run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "    spark_processor.run(\n",
    "        submit_app=\"./code/process.py\",\n",
    "        arguments=[\n",
    "             \"--s3_dataset_path\",\n",
    "            s3_dataset_path_submissions,\n",
    "            \"--s3_output_bucket\",\n",
    "            bucket,\n",
    "            \"--s3_output_prefix\",\n",
    "            output_prefix_data_submissions,\n",
    "            \"--col_name_for_filtering\",\n",
    "            col_name_for_filtering,\n",
    "            \"--values_to_keep\",\n",
    "            subreddits,\n",
    "        ],\n",
    "        spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "        logs=False,\n",
    "        configuration=configuration\n",
    "    )\n",
    "    # give some time for resources from this iterations to get cleaned up\n",
    "    # if we start the job immediately we could get insufficient resources error\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the filtered data\n",
    "\n",
    "Now that we have filtered the data to only keep submissions and comments from subreddits of interest. Let us read data from the s3 path where we saved the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://sagemaker-us-east-1-163140600147/project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "public_bucket = 'sagemaker-us-east-1-163140600147'\n",
    "output_prefix = 'project/comments/'\n",
    "s3_input_path_comments = f\"s3a://{public_bucket}/{output_prefix}\"\n",
    "comments = spark.read.parquet(s3_input_path_comments)\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check counts (ensuring all needed subreddits exist)\n",
    "comments.groupBy('subreddit').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display a subset of columns\n",
    "comments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\", \"score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "public_bucket = 'sagemaker-us-east-1-163140600147'\n",
    "output_prefix = 'project/submissions/'\n",
    "s3_input_path_submissions = f\"s3a://{public_bucket}/{output_prefix}\"\n",
    "submissions = spark.read.parquet(s3_input_path_submissions)\n",
    "print(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check counts (ensuring all needed subreddits exist)\n",
    "submissions.groupBy('subreddit').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display a subset of columns\n",
    "submissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"created_utc\", \"num_comments\", \"score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter \"AskWomen\", \"AskFeminists\", \"Feminism\" by STEM Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Subreddits to filter by keywords\n",
    "keyword_subreddits = [\"AskWomen\", \"AskFeminists\", \"Feminism\"]\n",
    "# Subreddits to include all comments from\n",
    "include_all_subreddits = [\"xxstem\", \"LadiesofScience\", \"womenEngineers\"]\n",
    "\n",
    "# Define keywords for case-insensitive search\n",
    "keywords = [\"STEM\", \"Science\", \"Technology\", \"Engineering\", \"Mathematics\", \"Process\", \"Design\", \"Model\", \"Plan\", \"Project\"]\n",
    "keywords_lower = [kw.lower() for kw in keywords]\n",
    "\n",
    "# Filter the DataFrame\n",
    "comments = comments.filter(\n",
    "    (col(\"subreddit\").isin(keyword_subreddits) & col(\"body\").rlike('|'.join(keywords_lower))) |\n",
    "    (col(\"subreddit\").isin(include_all_subreddits))\n",
    ")\n",
    "\n",
    "# Show the filtered data\n",
    "comments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\", \"score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Subreddits to filter by keywords\n",
    "keyword_subreddits = [\"AskWomen\", \"AskFeminists\", \"Feminism\"]\n",
    "# Subreddits to include all submissions from\n",
    "include_all_subreddits = [\"xxstem\", \"LadiesofScience\", \"womenEngineers\"]\n",
    "\n",
    "# Define keywords for case-insensitive search\n",
    "keywords = [\"STEM\", \"science\", \"technology\", \"engineering\", \"mathematics\", \"process\", \"design\", \"model\", \"plan\", \"project\"]\n",
    "# Create a regex pattern to match any keyword (case-insensitive)\n",
    "pattern = '|'.join([f\"(?i){kw}\" for kw in keywords])\n",
    "\n",
    "# Filter the DataFrame\n",
    "# Include all submissions from certain subreddits or those that match the keyword pattern in their title or selftext\n",
    "submissions = submissions.filter(\n",
    "    (col(\"subreddit\").isin(keyword_subreddits) & (col(\"title\").rlike(pattern) | col(\"selftext\").rlike(pattern))) |\n",
    "    col(\"subreddit\").isin(include_all_subreddits)\n",
    ")\n",
    "\n",
    "# Show the filtered data\n",
    "submissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"created_utc\", \"num_comments\", \"score\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove rows with missing values in 'author' and 'body'\n",
    "comments = comments.filter(col(\"author\").isNotNull() & col(\"body\").isNotNull())\n",
    "\n",
    "# Assume that 'created_utc' should be a timestamp within the last 3 years\n",
    "three_years_ago = unix_timestamp(current_timestamp()) - (3 * 365 * 24 * 60 * 60)\n",
    "comments = comments.filter(\n",
    "    unix_timestamp(col(\"created_utc\")) > three_years_ago\n",
    ")\n",
    "\n",
    "# Show the filtered data\n",
    "comments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\", \"score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove rows with missing values in 'author' or 'title'\n",
    "submissions = submissions.filter(col(\"author\").isNotNull() & col(\"title\").isNotNull())\n",
    "\n",
    "# Assume that 'created_utc' should be a timestamp within the last 3 years\n",
    "three_years_ago = unix_timestamp(current_timestamp()) - (3 * 365 * 24 * 60 * 60)\n",
    "submissions = submissions.filter(\n",
    "    unix_timestamp(col(\"created_utc\")) > three_years_ago\n",
    ")\n",
    "\n",
    "# Show the filtered data\n",
    "submissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"created_utc\", \"num_comments\",  \"score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table: Subreddit Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Count the number of submissions per subreddit\n",
    "submissions_count = submissions.groupBy(\"subreddit\").count().withColumnRenamed(\"count\", \"Submissions\")\n",
    "\n",
    "# Count the number of comments per subreddit\n",
    "comments_count = comments.groupBy(\"subreddit\").count().withColumnRenamed(\"count\", \"Comments\")\n",
    "\n",
    "# Join the counts on subreddit\n",
    "subreddit_data = submissions_count.join(comments_count, \"subreddit\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "subreddit_data_pd = subreddit_data.toPandas()\n",
    "\n",
    "# Function to create and save a table image using tabulate\n",
    "def save_table_image(df, title, filename):\n",
    "    table = tabulate(df, headers='keys', tablefmt='psql', showindex=False)\n",
    "    lines = table.split('\\n')\n",
    "    fig_width = min(max([len(line) for line in lines]) / 2, 10)\n",
    "    fig_height = min(len(lines) / 2, 10)\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "    plt.text(0.5, 0.5, table, horizontalalignment='center', verticalalignment='center', fontsize=12, family='monospace')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16, weight='bold')\n",
    "    plt.savefig(filename, bbox_inches='tight', pad_inches=0.05)\n",
    "    plt.close()\n",
    "\n",
    "# Define the title for the image and the file path\n",
    "title = \"Subreddit Submission and Comment Counts\"\n",
    "file_path = 'subreddit_counts_table.png'\n",
    "\n",
    "# Create and save the table image\n",
    "save_table_image(subreddit_data_pd, title, file_path)\n",
    "\n",
    "# Show the table\n",
    "print(subreddit_data_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table: Top 10 Authors in Submissions and Comments Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out \"[deleted]\" authors from comments and submissions before aggregation\n",
    "filtered_comments = comments.filter(col(\"author\") != \"[deleted]\")\n",
    "filtered_submissions = submissions.filter(col(\"author\") != \"[deleted]\")\n",
    "\n",
    "# Find the most active users in filtered comments\n",
    "most_active_users_comments = (\n",
    "    filtered_comments.groupBy(\"author\")\n",
    "    .agg(count(\"*\").alias(\"comments_count\"))\n",
    "    .orderBy(col(\"comments_count\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "# Find the most active users in filtered submissions\n",
    "most_active_users_submissions = (\n",
    "    filtered_submissions.groupBy(\"author\")\n",
    "    .agg(count(\"*\").alias(\"submissions_count\"))\n",
    "    .orderBy(col(\"submissions_count\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "# # find the most active users in comments\n",
    "# most_active_users_comments = comments.groupBy(\"author\").agg(count(\"*\").alias(\"comments_count\")).orderBy(\"comments_count\", ascending=False).limit(10)\n",
    "\n",
    "# # find the most active users in submissions\n",
    "# most_active_users_submissions = submissions.groupBy(\"author\").agg(count(\"*\").alias(\"submissions_count\")).orderBy(\"submissions_count\", ascending=False).limit(10)\n",
    "\n",
    "# Converting to Pandas\n",
    "top10_author_submissions = most_active_users_submissions.toPandas()\n",
    "top10_author_comments = most_active_users_comments.toPandas()\n",
    "\n",
    "# Change colulmn names\n",
    "top10_author_submissions.columns = ['Author', 'Submissions']\n",
    "top10_author_comments.columns = ['Author', 'Comments']\n",
    "\n",
    "# Using tabulate to print out the tables in a nicely formatted way\n",
    "table_submissions = tabulate(top10_author_submissions, headers='keys', tablefmt='psql', showindex=False)\n",
    "table_comments = tabulate(top10_author_comments, headers='keys', tablefmt='psql', showindex=False)\n",
    "\n",
    "def save_table_image(table, title, filename):\n",
    "    # Split the table into lines\n",
    "    lines = table.split('\\n')\n",
    "    # Estimate the size of the image\n",
    "    fig_width = max([len(line) for line in lines]) / 2  # adjust the division factor as needed\n",
    "    fig_height = len(lines) / 2  # adjust the division factor as needed\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "    plt.text(0.5, 0.5, table, horizontalalignment='center', verticalalignment='center', fontsize=12, family='monospace')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16, weight='bold')\n",
    "    plt.savefig(filename, bbox_inches='tight', pad_inches=0.05)\n",
    "    plt.close()\n",
    "\n",
    "# Save the tables as images\n",
    "save_table_image(table_submissions, 'Top 10 Authors by Submissions', 'top10_author_submissions.png')\n",
    "save_table_image(table_comments, 'Top 10 Authors by Comments', 'top10_author_comments.png')\n",
    "\n",
    "# Display the tables as tabulate tables\n",
    "print(table_submissions)\n",
    "print(table_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table: Common authors in Comments and Submissions Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, sum as sum_agg\n",
    "\n",
    "# Filter out \"[deleted]\" authors\n",
    "filtered_comments = comments.filter(col(\"author\") != \"[deleted]\")\n",
    "filtered_submissions = submissions.filter(col(\"author\") != \"[deleted]\")\n",
    "\n",
    "# Aggregate count and score for comments\n",
    "comment_agg = filtered_comments.groupBy(\"author\").agg(\n",
    "    count(\"id\").alias(\"num_comments\"),\n",
    "    sum_agg(\"score\").alias(\"total_comment_score\")\n",
    ")\n",
    "\n",
    "# Aggregate count and score for submissions\n",
    "submission_agg = filtered_submissions.groupBy(\"author\").agg(\n",
    "    count(\"*\").alias(\"num_submissions\"),\n",
    "    sum_agg(\"score\").alias(\"total_submission_score\")\n",
    ")\n",
    "\n",
    "# Perform inner join and limit to top 20 common authors\n",
    "common_authors = (\n",
    "    comment_agg.join(\n",
    "        submission_agg,\n",
    "        \"author\"\n",
    "    )\n",
    "    .orderBy(col(\"num_comments\").desc(), col(\"num_submissions\").desc())\n",
    "    .limit(20)\n",
    ")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "common_authors_pd = common_authors.toPandas()\n",
    "\n",
    "# Updating the DataFrame column names\n",
    "common_authors_pd.columns = [\"Author\", \"Number of Comments\", \"Total Comment Score\", \"Number of Submissions\", \"Total Submission Score\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Filter out \"[deleted]\" authors from comments and submissions before aggregation\n",
    "# filtered_comments = comments.filter(col(\"author\") != \"[deleted]\")\n",
    "# filtered_submissions = submissions.filter(col(\"author\") != \"[deleted]\")\n",
    "\n",
    "# # Perform inner join and limit to top 20 common authors\n",
    "# common_authors = (\n",
    "#     filtered_comments.groupBy(\"author\").agg(count(\"id\").alias(\"num_comments\"))\n",
    "#     .join(\n",
    "#         filtered_submissions.groupBy(\"author\").agg(count(\"*\").alias(\"num_submissions\")),\n",
    "#         \"author\"\n",
    "#     )\n",
    "#     .orderBy(col(\"num_comments\").desc(), col(\"num_submissions\").desc())\n",
    "#     .limit(20)\n",
    "# )\n",
    "\n",
    "# # Convert to Pandas DataFrame\n",
    "# common_authors_pd = common_authors.toPandas()\n",
    "\n",
    "# # Updating the column names for the DataFrame\n",
    "# common_authors_pd.columns = [\"Author\", \"Number of Comments\", \"Number of Submissions\"]\n",
    "\n",
    "# Function to save the DataFrame as an image using tabulate\n",
    "def save_table_image(df, title, filename):\n",
    "    table = tabulate(df, headers='keys', tablefmt='psql', showindex=False)\n",
    "    lines = table.split('\\n')\n",
    "    fig_width = min(max([len(line) for line in lines]) / 2, 10)\n",
    "    fig_height = min(len(lines) / 2, 10)\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "    plt.text(0.5, 0.5, table, horizontalalignment='center', verticalalignment='center', fontsize=12, family='monospace')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16, weight='bold')\n",
    "    plt.savefig(filename, bbox_inches='tight', pad_inches=0.05)\n",
    "    plt.close()\n",
    "\n",
    "# Define the title for the image and the file path\n",
    "title = \"Top 20 Common Authors by Comments and Submissions\"\n",
    "file_path = 'top20_common_authors_table.png'\n",
    "\n",
    "# Create and save the table image\n",
    "save_table_image(common_authors_pd, title, file_path)\n",
    "\n",
    "# Display the table as output\n",
    "print(tabulate(common_authors_pd, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table: Length of the comments and submissions (Minimum, Maximum, Mean, Median, Standard Deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate length statistics for comments\n",
    "comments = comments.withColumn('body_length', length(col('body')))\n",
    "comments_stats = comments.select(\n",
    "    F.min(col('body_length')).alias('Minimum Length'),\n",
    "    F.max(col('body_length')).alias('Maximum Length'),\n",
    "    F.avg(col('body_length')).alias('Average Length'),\n",
    "    F.expr('percentile_approx(body_length, 0.5)').alias('Median Length'),\n",
    "    F.stddev(col('body_length')).alias('Standard Deviation')\n",
    ").collect()[0].asDict()\n",
    "\n",
    "# Calculate length statistics for submissions\n",
    "submissions = submissions.withColumn('selftext_length', length(col('selftext')))\n",
    "submissions_stats = submissions.select(\n",
    "    F.min(col('selftext_length')).alias('Minimum Length'),\n",
    "    F.max(col('selftext_length')).alias('Maximum Length'),\n",
    "    F.avg(col('selftext_length')).alias('Average Length'),\n",
    "    F.expr('percentile_approx(selftext_length, 0.5)').alias('Median Length'),\n",
    "    F.stddev(col('selftext_length')).alias('Standard Deviation')\n",
    ").collect()[0].asDict()\n",
    "\n",
    "# Convert stats to pandas dataframes\n",
    "comments_stats_df = pd.DataFrame([comments_stats])\n",
    "submissions_stats_df = pd.DataFrame([submissions_stats])\n",
    "\n",
    "def save_table_as_image(df, title, file_name):\n",
    "    # Convert the DataFrame to a table using tabulate\n",
    "    table = tabulate(df, headers='keys', tablefmt='psql', showindex=False)\n",
    "\n",
    "    # Split the table into lines to estimate its size\n",
    "    lines = table.split('\\n')\n",
    "    fig_width = max([len(line) for line in lines]) / 2  # adjust the division factor as needed\n",
    "    fig_height = len(lines) / 2  # adjust the division factor as needed\n",
    "\n",
    "    # Create figure with dynamic size\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))  # Size may need to be adjusted\n",
    "    ax.text(0.5, 0.5, table, horizontalalignment='center', verticalalignment='center', fontsize=12, family='monospace')\n",
    "    ax.axis('off')\n",
    "    plt.title(title, fontsize=16, weight='bold')\n",
    "\n",
    "    # Save the figure to a file\n",
    "    plt.savefig(f\"{file_name}.png\", bbox_inches='tight', pad_inches=0.05)\n",
    "    plt.close()\n",
    "\n",
    "# Convert stats to pandas dataframes if not already done\n",
    "comments_stats_df = pd.DataFrame([comments_stats])\n",
    "submissions_stats_df = pd.DataFrame([submissions_stats])\n",
    "\n",
    "# Save the tables as images with tabulate formatting\n",
    "save_table_as_image(comments_stats_df, \"Comments Length Statistics\", \"comments_length_stats\")\n",
    "save_table_as_image(submissions_stats_df, \"Submissions Length Statistics\", \"submissions_length_stats\")\n",
    "\n",
    "# Paths to the saved images\n",
    "# comments_length_stats_path = \"comments_length_stats.png\"\n",
    "# submissions_length_stats_path = \"submissions_length_stats.png\"\n",
    "\n",
    "# Display the table as output\n",
    "print(\"Comments Length Statistics:\")\n",
    "print(tabulate(comments_stats_df, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"\\nSubmissions Length Statistics:\")\n",
    "print(tabulate(submissions_stats_df, headers='keys', tablefmt='psql', showindex=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart: Time analysis Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Process submissions DataFrame\n",
    "submissions_df = submissions.withColumn(\"date\", to_date(col(\"created_utc\")))\n",
    "submissions_count_by_date = submissions_df.groupBy(\"date\").count().orderBy(\"date\")\n",
    "\n",
    "# Process comments DataFrame\n",
    "comments_df = comments.withColumn(\"date\", to_date(col(\"created_utc\")))\n",
    "comments_count_by_date = comments_df.groupBy(\"date\").count().orderBy(\"date\")\n",
    "\n",
    "# Convert to Pandas DataFrames for plotting\n",
    "submissions_pd = submissions_count_by_date.toPandas()\n",
    "comments_pd = comments_count_by_date.toPandas()# Create traces for the chart\n",
    "trace_submissions = go.Scatter(x=submissions_pd['date'], y=submissions_pd['count'], name='Submissions')\n",
    "trace_comments = go.Scatter(x=comments_pd['date'], y=comments_pd['count'], name='Comments', visible=False)\n",
    "\n",
    "# Create the figure with both traces\n",
    "fig = go.Figure(data=[trace_submissions, trace_comments])\n",
    "\n",
    "# Define the buttons for the dropdown menu\n",
    "button_submissions = dict(label='Submissions', method='update',\n",
    "                          args=[{'visible': [True, False]}, {'title': 'Number of Submissions Over Time'}])\n",
    "button_comments = dict(label='Comments', method='update',\n",
    "                       args=[{'visible': [False, True]}, {'title': 'Number of Comments Over Time'}])\n",
    "\n",
    "# Add dropdown menus to the figure\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(active=0, buttons=[button_submissions, button_comments])]\n",
    ")\n",
    "\n",
    "# Add a range slider for the time dimension\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1, label='1m', step='month', stepmode='backward'),\n",
    "                dict(count=6, label='6m', step='month', stepmode='backward'),\n",
    "                dict(count=1, label='1y', step='year', stepmode='backward'),\n",
    "                dict(count=1, label='YTD', step='year', stepmode='todate'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(visible=True),\n",
    "        type='date'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set the layout for the figure\n",
    "fig.update_layout(title='Interactive Time Series Chart', xaxis_title='Date', yaxis_title='Count')\n",
    "\n",
    "# # Save the interactive plot to an HTML file\n",
    "fig.write_html(\"timeseries.html\")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Define the subreddits\n",
    "subreddits = [\"xxstem\", \"Feminism\", \"LadiesofScience\", \"womenEngineers\", \"AskFeminists\", \"AskWomen\"]\n",
    "\n",
    "# Process submissions DataFrame\n",
    "submissions_df = submissions.withColumn(\"date\", to_date(col(\"created_utc\")))\n",
    "submissions_count_by_date_subreddit = submissions_df.groupBy(\"date\", \"subreddit\").count().orderBy(\"date\")\n",
    "\n",
    "# Process comments DataFrame\n",
    "comments_df = comments.withColumn(\"date\", to_date(col(\"created_utc\")))\n",
    "comments_count_by_date_subreddit = comments_df.groupBy(\"date\", \"subreddit\").count().orderBy(\"date\")\n",
    "\n",
    "# Convert to Pandas DataFrames\n",
    "submissions_pd = submissions_count_by_date_subreddit.toPandas()\n",
    "comments_pd = comments_count_by_date_subreddit.toPandas()\n",
    "\n",
    "# Create traces for Submissions and Comments\n",
    "traces_submissions = []\n",
    "traces_comments = []\n",
    "for subreddit in subreddits:\n",
    "    # Traces for Submissions\n",
    "    trace_submissions = go.Scatter(\n",
    "        x=submissions_pd[submissions_pd['subreddit'] == subreddit]['date'],\n",
    "        y=submissions_pd[submissions_pd['subreddit'] == subreddit]['count'],\n",
    "        name=f'{subreddit}'\n",
    "    )\n",
    "    traces_submissions.append(trace_submissions)\n",
    "\n",
    "    # Traces for Comments\n",
    "    trace_comments = go.Scatter(\n",
    "        x=comments_pd[comments_pd['subreddit'] == subreddit]['date'],\n",
    "        y=comments_pd[comments_pd['subreddit'] == subreddit]['count'],\n",
    "        name=f'{subreddit}',\n",
    "        visible=False  # Initially hidden\n",
    "    )\n",
    "    traces_comments.append(trace_comments)\n",
    "\n",
    "# Create the figure with all traces\n",
    "fig = go.Figure(data=traces_submissions + traces_comments)\n",
    "\n",
    "# Define the buttons for the dropdown menu\n",
    "buttons = [\n",
    "    dict(\n",
    "        label='Submissions',\n",
    "        method='update',\n",
    "        args=[{'visible': [True]*len(traces_submissions) + [False]*len(traces_comments)},\n",
    "              {'title': 'Number of Submissions Over Time'}]\n",
    "    ),\n",
    "    dict(\n",
    "        label='Comments',\n",
    "        method='update',\n",
    "        args=[{'visible': [False]*len(traces_submissions) + [True]*len(traces_comments)},\n",
    "              {'title': 'Number of Comments Over Time'}]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Add dropdown menus to the figure\n",
    "fig.update_layout(updatemenus=[dict(active=0, buttons=buttons)])\n",
    "\n",
    "# Add a range slider and other layout configurations\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1, label='1m', step='month', stepmode='backward'),\n",
    "                dict(count=6, label='6m', step='month', stepmode='backward'),\n",
    "                dict(count=1, label='1y', step='year', stepmode='backward'),\n",
    "                dict(count=1, label='YTD', step='year', stepmode='todate'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(visible=True),\n",
    "        type='date'\n",
    "    ),\n",
    "    title='Interactive Time Series Chart by Subreddit and Content Type',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Count'\n",
    ")\n",
    "\n",
    "# Save the interactive plot to an HTML file\n",
    "fig.write_html(\"timeseries_comments_submissions.html\")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "# Extract hour from timestamp\n",
    "submissions_df = submissions.withColumn(\"hour\", hour(\"created_utc\"))\n",
    "comments_df = comments.withColumn(\"hour\", hour(\"created_utc\"))\n",
    "\n",
    "# Group by hour and count\n",
    "submissions_hourly = submissions_df.groupBy(\"hour\").count().orderBy(\"hour\")\n",
    "comments_hourly = comments_df.groupBy(\"hour\").count().orderBy(\"hour\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "submissions_hourly_pd = submissions_hourly.toPandas()\n",
    "comments_hourly_pd = comments_hourly.toPandas()\n",
    "\n",
    "# Convert hour to string and format\n",
    "submissions_hourly_pd['hour'] = submissions_hourly_pd['hour'].apply(lambda x: f'{x:02d}:00')\n",
    "comments_hourly_pd['hour'] = comments_hourly_pd['hour'].apply(lambda x: f'{x:02d}:00')\n",
    "\n",
    "# Create traces for the chart\n",
    "trace_submissions = go.Scatter(x=submissions_hourly_pd['hour'], y=submissions_hourly_pd['count'], name='Submissions')\n",
    "trace_comments = go.Scatter(x=comments_hourly_pd['hour'], y=comments_hourly_pd['count'], name='Comments')\n",
    "\n",
    "# Create the figure with both traces\n",
    "fig = go.Figure(data=[trace_submissions, trace_comments])\n",
    "\n",
    "# Define the buttons for the dropdown menu\n",
    "button_submissions = dict(label='Submissions', method='update', args=[{'visible': [True, False]}, {'title': 'Hourly Submissions'}])\n",
    "button_comments = dict(label='Comments', method='update', args=[{'visible': [False, True]}, {'title': 'Hourly Comments'}])\n",
    "\n",
    "# Add dropdown menus to the figure\n",
    "fig.update_layout(updatemenus=[dict(active=0, buttons=[button_submissions, button_comments])])\n",
    "\n",
    "# Set the layout for the figure\n",
    "fig.update_layout(title='Interactive Hourly Analysis', xaxis_title='Hour of the Day', yaxis_title='Count')\n",
    "\n",
    "fig.write_html(\"timeseries_hourly_analysis.html\")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "# Add 'day_of_week' column (1 = Sunday, 2 = Monday, ..., 7 = Saturday)\n",
    "submissions = submissions.withColumn(\"day_of_week\", dayofweek(\"created_utc\"))\n",
    "comments = comments.withColumn(\"day_of_week\", dayofweek(\"created_utc\"))\n",
    "\n",
    "# Group by 'day_of_week' and count\n",
    "submissions_day_of_week = submissions.groupBy(\"day_of_week\").count().orderBy(\"day_of_week\")\n",
    "comments_day_of_week = comments.groupBy(\"day_of_week\").count().orderBy(\"day_of_week\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "submissions_day_of_week_pd = submissions_day_of_week.toPandas()\n",
    "comments_day_of_week_pd = comments_day_of_week.toPandas()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces for submissions and comments\n",
    "trace_submissions = go.Bar(\n",
    "    x=submissions_day_of_week_pd['day_of_week'],\n",
    "    y=submissions_day_of_week_pd['count'],\n",
    "    name='Submissions'\n",
    ")\n",
    "\n",
    "trace_comments = go.Bar(\n",
    "    x=comments_day_of_week_pd['day_of_week'],\n",
    "    y=comments_day_of_week_pd['count'],\n",
    "    name='Comments'\n",
    ")\n",
    "\n",
    "# Create the figure with both traces\n",
    "fig = go.Figure(data=[trace_submissions, trace_comments])\n",
    "\n",
    "# Define the buttons for the dropdown menu\n",
    "button_submissions = dict(\n",
    "    label='Submissions',\n",
    "    method='update',\n",
    "    args=[{'visible': [True, False]},\n",
    "          {'title': 'Number of Submissions by Day of the Week'}]\n",
    ")\n",
    "\n",
    "button_comments = dict(\n",
    "    label='Comments',\n",
    "    method='update',\n",
    "    args=[{'visible': [False, True]},\n",
    "          {'title': 'Number of Comments by Day of the Week'}]\n",
    ")\n",
    "\n",
    "# Add dropdown menus to the figure\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(active=0, buttons=[button_submissions, button_comments])]\n",
    ")\n",
    "\n",
    "# Set the layout for the figure\n",
    "fig.update_layout(\n",
    "    title='Day Wise Analysis of Submissions and Comments',\n",
    "    xaxis_title='Day of the Week',\n",
    "    yaxis_title='Count',\n",
    "    xaxis=dict(tickmode='array', tickvals=list(range(1, 8)), ticktext=['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'])\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(\"timeseries_day_analysis.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart: Content Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract comments text data\n",
    "text_data_comments = comments.select(\"body\").rdd.flatMap(lambda x: x).filter(lambda x: x is not None).collect()\n",
    "text_comments = \" \".join(text_data_comments)\n",
    "\n",
    "\n",
    "# extract submissions text data\n",
    "text_data_submissions = submissions.select(\"title\",\"selftext\").rdd.flatMap(lambda x: x).filter(lambda x: x is not None).collect()\n",
    "text_submissions = \" \".join(text_data_submissions)\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update(nltk_stopwords)\n",
    "stopwords.update([\"removed\", \"deleted\",\"one\", \"will\", \"using\", \"used\", \"also\"])\n",
    "\n",
    "\n",
    "# build wordcloud for comments\n",
    "wordcloud_comments = WordCloud(width=800, height=400, background_color ='white', stopwords=stopwords).generate(text_comments)\n",
    "# build wordcloud for submissions\n",
    "wordcloud_submissions = WordCloud(width=800, height=400, background_color ='white', stopwords=stopwords).generate(text_submissions)\n",
    "\n",
    "# plot wordcloud\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud_comments)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\"wordcloud_comments.png\", bbox_inches='tight', dpi=300)  \n",
    "print(\"Wordcloud for comments:\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud_submissions)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\"wordcloud_submissions.png\", bbox_inches='tight', dpi=300)  \n",
    "print(\"Wordcloud for submissions:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart: Correlation Chart: Comments dataset Number of Comments v/s Submissions dataset Length of Self Text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the Spark DataFrame to a Pandas DataFrame for visualization purposes\n",
    "submissions_pd = submissions.select(\"num_comments\", \"selftext_length\").toPandas()\n",
    "\n",
    "correlation = submissions.stat.corr(\"num_comments\", \"selftext_length\")\n",
    "\n",
    "# Create an interactive scatter plot\n",
    "fig = px.scatter(submissions_pd, x=\"selftext_length\", y=\"num_comments\",\n",
    "                 title=f\"Correlation between Number of Comments and Length of Selftext: {correlation:.2f}\",\n",
    "                 labels={\"selftext_length\": \"Length of Self-text\", \"num_comments\": \"Number of Comments\"})\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# Save the interactive plot to an HTML file (optional)\n",
    "fig.write_html(\"correlation_analysis_interactive.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart: Top Authors with Most Comments and Most Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chart2_submissions = submissions.filter(col(\"author\") != \"[deleted]\")\n",
    "# chart2_comments = comments.filter(col(\"author\") != \"[deleted]\")\n",
    "\n",
    "# # Count the number of submissions per author\n",
    "# author_submission_counts = chart2_submissions.groupBy('author').count().withColumnRenamed('count', 'submission_count')\n",
    "\n",
    "# # Count the number of comments per author\n",
    "# author_comment_counts = chart2_comments.groupBy('author').count().withColumnRenamed('count', 'comment_count')\n",
    "\n",
    "# # Get the top authors based on submission count\n",
    "# top_authors_submissions_pd = author_submission_counts.orderBy(desc('submission_count')).limit(10).toPandas()\n",
    "\n",
    "# # Get the top authors based on comment count\n",
    "# top_authors_comments_pd = author_comment_counts.orderBy(desc('comment_count')).limit(10).toPandas()\n",
    "\n",
    "# # Create an interactive bar chart for top authors by submissions\n",
    "# fig_submissions = px.bar(\n",
    "#     top_authors_submissions_pd,\n",
    "#     x='author',\n",
    "#     y='submission_count',\n",
    "#     title='Top Authors by Submissions',\n",
    "#     labels={'submission_count': 'Number of Submissions', 'author': 'Author'}\n",
    "# )\n",
    "# fig_submissions.update_layout(xaxis_tickangle=-45)\n",
    "# fig_submissions.write_html(\"top_authors_submissions_interactive.html\")\n",
    "\n",
    "# # Create an interactive bar chart for top authors by comments\n",
    "# fig_comments = px.bar(\n",
    "#     top_authors_comments_pd,\n",
    "#     x='author',\n",
    "#     y='comment_count',\n",
    "#     title='Top Authors by Comments',\n",
    "#     labels={'comment_count': 'Number of Comments', 'author': 'Author'}\n",
    "# )\n",
    "# fig_comments.update_layout(xaxis_tickangle=-45)\n",
    "# fig_comments.write_html(\"top_authors_comments_interactive.html\")\n",
    "\n",
    "# # Paths to the saved interactive charts\n",
    "# submissions_chart_path = \"top_authors_submissions_interactive.html\"\n",
    "# comments_chart_path = \"top_authors_comments_interactive.html\"\n",
    "\n",
    "# # Display the interactive charts\n",
    "# fig_submissions.show()\n",
    "# fig_comments.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a figure with a dropdown menu\n",
    "fig = make_subplots()\n",
    "\n",
    "# Trace for top authors by submissions\n",
    "trace_submissions = go.Bar(\n",
    "    x=top_authors_submissions_pd['author'],\n",
    "    y=top_authors_submissions_pd['submission_count'],\n",
    "    name='Submissions'\n",
    ")\n",
    "\n",
    "# Trace for top authors by comments\n",
    "trace_comments = go.Bar(\n",
    "    x=top_authors_comments_pd['author'],\n",
    "    y=top_authors_comments_pd['comment_count'],\n",
    "    name='Comments'\n",
    ")\n",
    "\n",
    "# Add traces to the figure\n",
    "fig.add_trace(trace_submissions, 1, 1)\n",
    "fig.add_trace(trace_comments, 1, 1)\n",
    "\n",
    "# Make the comments trace invisible initially\n",
    "fig.data[1].visible = False\n",
    "\n",
    "# Create a dropdown menu for switching between traces\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args=[{\"visible\": [True, False]}],\n",
    "                    label=\"Submissions\",\n",
    "                    method=\"update\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{\"visible\": [False, True]}],\n",
    "                    label=\"Comments\",\n",
    "                    method=\"update\"\n",
    "                )\n",
    "            ]),\n",
    "            direction=\"down\",\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=1,\n",
    "            xanchor=\"left\",\n",
    "            y=1.3,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set the layout for the figure\n",
    "fig.update_layout(\n",
    "    title_text=\"Top Authors by Submissions and Comments\",\n",
    "    xaxis_title=\"Author\",\n",
    "    yaxis_title=\"Count\",\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "\n",
    "# Save the interactive plot to an HTML file\n",
    "fig.write_html(\"top_authors_interactive.html\")\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart: Engagement Metrics of Number of Comments per Submissions (For each subrredit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# Group by 'subreddit' and collect the 'num_comments' into a list\n",
    "subreddit_comments = submissions.groupBy('subreddit').agg(collect_list('num_comments').alias('comments'))\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "subreddit_comments_pd = subreddit_comments.toPandas().set_index('subreddit')\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces (one for each subreddit), but set them to be not visible initially\n",
    "for subreddit, data in subreddit_comments_pd.iterrows():\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=data['comments'],\n",
    "            name=subreddit,\n",
    "            opacity=0.75,\n",
    "            visible=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create a dropdown menu with options for each subreddit\n",
    "subreddit_options = [{\"label\": subreddit, \"method\": \"update\", \"args\": [{\"visible\": [subreddit == r for r in subreddit_comments_pd.index]}]} for subreddit in subreddit_comments_pd.index]\n",
    "\n",
    "# Update the layout to add the dropdown\n",
    "fig.update_layout(\n",
    "    title='Number of Comments Distribution per Subreddit',\n",
    "    xaxis_title='Number of Comments',\n",
    "    yaxis_title='Frequency',\n",
    "    updatemenus=[{\n",
    "        \"buttons\": subreddit_options,\n",
    "        \"direction\": \"down\",\n",
    "        \"showactive\": True,\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Set the first trace to be visible initially\n",
    "fig.data[0].visible = True\n",
    "\n",
    "# Show the interactive figure\n",
    "fig.show()\n",
    "\n",
    "# Save the interactive plot to an HTML file\n",
    "fig.write_html(\"subreddit_comments_distribution_interactive.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart: Issues faced by Women\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the issues and corresponding regex patterns\n",
    "issues = {\n",
    "    \"Workplace Discrimination\": \"workplace|discrimination\",\n",
    "    \"Gender Equality\": \"gender equality|feminism\",\n",
    "    \"Healthcare\": \"healthcare|health care|reproductive rights\",\n",
    "    \"Color\": \"hispanic|black|asian|ethnicity\",\n",
    "    \"Domestic Violence\": \"domestic violence|abuse\",\n",
    "    \"Parental Leave\": \"maternity leave|paternity leave|parental leave\",\n",
    "    \"Body Image\": \"body image|body positivity\",\n",
    "    \"Education\": \"education|school|university\",\n",
    "    \"Career Advancement\": \"promotion|career advancement\",\n",
    "    \"Harassment\": \"harassment|sexual harassment\",\n",
    "    \"Mental Health\": \"mental health|depression|anxiety\",\n",
    "    \"Equal Pay\": \"equal pay|wage gap\",\n",
    "    \"Political Representation\": \"politics|representation\",\n",
    "    \"Child Care\": \"child care|childcare|babysitting\",\n",
    "    \"Menstruation\": \"menstruation|period|menstrual\",\n",
    "    \"LGBTQ+ Rights\": \"LGBTQ|lesbian|gay|bisexual|transgender\",\n",
    "    \"Marriage and Divorce\": \"marriage|divorce|wedding\",\n",
    "    \"Aging\": \"aging|elderly|senior\",\n",
    "    \"Fashion and Beauty Standards\": \"fashion|beauty standards|makeup\",\n",
    "    \"Sexual and Reproductive Health\": \"contraception|birth control|abortion\"\n",
    "}\n",
    "\n",
    "# Initialize dictionaries for counting occurrences\n",
    "issue_counts_submissions = {issue: 0 for issue in issues.keys()}\n",
    "issue_counts_comments = {issue: 0 for issue in issues.keys()}\n",
    "\n",
    "# Function to count occurrences of each issue\n",
    "def count_issues(df, column, issue_counts):\n",
    "    for issue, pattern in issues.items():\n",
    "        count = df.filter(col(column).rlike(pattern)).count()\n",
    "        issue_counts[issue] += count\n",
    "\n",
    "# Count issues in comments and submissions\n",
    "count_issues(comments, \"body\", issue_counts_comments)\n",
    "count_issues(submissions, \"title\", issue_counts_submissions)\n",
    "count_issues(submissions, \"selftext\", issue_counts_submissions)\n",
    "\n",
    "labels = list(issues.keys())\n",
    "values_submissions = list(issue_counts_submissions.values())\n",
    "values_comments = list(issue_counts_comments.values())\n",
    "\n",
    "color_palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "                 '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf',\n",
    "                 ]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Submissions\", \"Comments\"))\n",
    "\n",
    "fig.add_trace(go.Pie(labels=labels, values=values_submissions, name=\"Submissions\",\n",
    "                     marker=dict(colors=color_palette)), 1, 1)\n",
    "\n",
    "fig.add_trace(go.Pie(labels=labels, values=values_comments, name=\"Comments\",\n",
    "                     marker=dict(colors=color_palette)), 1, 2)\n",
    "\n",
    "fig.update_layout(title_text=\"Issues Faced by Women in Reddit\")\n",
    "fig.show()\n",
    "# Save the interactive plot to an HTML file\n",
    "fig.write_html(\"piechart.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: The work has been by us. However, we have taken assissstance from ChatGPT for code commenting and cleaning."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
